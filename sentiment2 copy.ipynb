{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('imdb_master.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnamed leftmost column\n",
    "df = df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out 'unsup' rows\n",
    "df = df[df['label'] != 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first 100 reviews\n",
    "df_sample = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into features and labels\n",
    "X = df_sample['review']\n",
    "y = df_sample['label'].apply(lambda x: 1 if x == 'pos' else 0)  # Convert labels to binary (1 for positive, 0 for negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    5055\n",
      "1    4945\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of labels\n",
    "print(y.value_counts())  # Ensure there are both 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance (if any)\n",
    "df_combined = pd.concat([X, y], axis=1)\n",
    "df_majority = df_combined[df_combined['label'] == 0]\n",
    "df_minority = df_combined[df_combined['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample the minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample with replacement\n",
    "                                 n_samples=len(df_majority), # to match majority class\n",
    "                                 random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate back into features and labels\n",
    "X_upsampled = df_upsampled['review']\n",
    "y_upsampled = df_upsampled['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and print metrics\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, zero_division=1)\n",
    "\n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "# Function to run sentiment analysis using the specified model\n",
    "def run_sentiment_analysis(X, y, model_name, model):\n",
    "    # Using traditional ML models\n",
    "    X_tfidf = tfidf.fit_transform(X)\n",
    "    model.fit(X_tfidf, y)\n",
    "    predictions = model.predict(X_tfidf)\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(y, predictions, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.93\n",
      "Precision: 0.92\n",
      "Recall: 0.95\n",
      "F1-Score: 0.93\n",
      "Confusion Matrix:\n",
      "[[4638  417]\n",
      " [ 276 4779]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      5055\n",
      "           1       0.92      0.95      0.93      5055\n",
      "\n",
      "    accuracy                           0.93     10110\n",
      "   macro avg       0.93      0.93      0.93     10110\n",
      "weighted avg       0.93      0.93      0.93     10110\n",
      "\n",
      "\n",
      "Naive Bayes Metrics:\n",
      "Accuracy: 0.89\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1-Score: 0.89\n",
      "Confusion Matrix:\n",
      "[[4528  527]\n",
      " [ 593 4462]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      5055\n",
      "           1       0.89      0.88      0.89      5055\n",
      "\n",
      "    accuracy                           0.89     10110\n",
      "   macro avg       0.89      0.89      0.89     10110\n",
      "weighted avg       0.89      0.89      0.89     10110\n",
      "\n",
      "\n",
      "Support Vector Machine Metrics:\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.99\n",
      "F1-Score: 0.99\n",
      "Confusion Matrix:\n",
      "[[5028   27]\n",
      " [  33 5022]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      5055\n",
      "           1       0.99      0.99      0.99      5055\n",
      "\n",
      "    accuracy                           0.99     10110\n",
      "   macro avg       0.99      0.99      0.99     10110\n",
      "weighted avg       0.99      0.99      0.99     10110\n",
      "\n",
      "\n",
      "Random Forest Metrics:\n",
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-Score: 1.00\n",
      "Confusion Matrix:\n",
      "[[5055    0]\n",
      " [   0 5055]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5055\n",
      "           1       1.00      1.00      1.00      5055\n",
      "\n",
      "    accuracy                           1.00     10110\n",
      "   macro avg       1.00      1.00      1.00     10110\n",
      "weighted avg       1.00      1.00      1.00     10110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run sentiment analysis with each model\n",
    "for model_name, model in models.items():\n",
    "    run_sentiment_analysis(X_upsampled, y_upsampled, model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
